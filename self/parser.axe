// Author: Navid Momtahen (C) 2025
// License: GPL-3.0
// 
// Handles the parsing process - converts tokens into an AST

use lexer (
    Token, 
    TokenType, lex
);

use structs (
    ASTNode
);

use std.string;
use std.io;
use std.maps;
use std.arena;

mut g_type_aliases: StringHashMapString;
mut g_imported_modules: StringHashMapBool;
mut g_list_of_types: StringHashMapString;
// mut g_macros: map(string, MacroDef);

def initialize_all() {
    mut arena: Arena = Arena.create(1024 * 1024);
    g_type_aliases = deref(StringHashMapString.create(ref_of(arena), 128));
    g_imported_modules = deref(StringHashMapBool.create(ref_of(arena), 128));
    g_list_of_types = deref(StringHashMapString.create(ref_of(arena), 128));
}


/// Parser context - holds state during parsing
model ParserContext {
    tokens: list(Token);
    pos: i32;
    is_axec: bool;
    check_entry_point: bool;
    current_module: string;
    current_scope: Scope;
}

/// Scope tracking for variable declarations
model Scope {
    variables: StringHashMapString;
    parent: ref Scope;
}

/// Parse the tokens into an AST
pub def parse(tokens: list(Token), is_axec: bool, check_entry_point: bool, current_module: string): ASTNode {
    mut ctx: ParserContext;
    ctx.tokens = tokens;
    ctx.pos = 0;
    ctx.is_axec = is_axec;
    ctx.check_entry_point = check_entry_point;
    ctx.current_module = current_module;
    
    mut ast: ASTNode;
    ast.node_type = str("Program");
    
    StringHashMapString.clear(addr_of(g_type_aliases));
    StringHashMapBool.clear(addr_of(g_imported_modules));
    StringHashMapString.clear(addr_of(g_list_of_types));
    
    if str_len(current_module) > 0 {
        // g_imported_modules[current_module] = true
        println "Auto-imported current module: ";
        println current_module;
    }
    
    loop {
        if ctx.pos >= len(tokens) {
            break;
        }
        skip_whitespace(addr_of(ctx));        
        if ctx.pos >= len(tokens) {
            break;
        }
        parse_top_level(addr_of(ctx), addr_of(ast));
    }
    
    return ast;
}

/// Parse a top-level construct (use, def, model, enum, val, mut val, etc.)
def parse_top_level(ctx: ref ParserContext, ast: ref ASTNode) {
    if ctx.pos >= len(ctx.tokens) {
        return;
    }
    
    val token_type = ctx.tokens.data[ctx.pos].token_type;
    
    // Handle USE statements
    if token_type == TokenType.USE {
        ctx.pos++;
        skip_whitespace(ctx);
        
        // TODO: Parse module path. 
        // For now, just skip until semicolon
        loop {
            if ctx.pos >= len(ctx.tokens) {
                break;
            }
            if ctx.tokens.data[ctx.pos].token_type == TokenType.SEMICOLON {
                ctx.pos++;
                break;
            }
            ctx.pos++;
        }
        return;
    }
    
    // Handle DEF (function definitions)
    if token_type == TokenType.DEF {
        ctx.pos++;
        skip_whitespace(ctx);
        
        if ctx.pos < len(ctx.tokens) and ctx.tokens.data[ctx.pos].token_type == TokenType.IDENTIFIER {
            ctx.pos++;
        }
        
        skip_whitespace(ctx);
        if ctx.pos < len(ctx.tokens) and ctx.tokens.data[ctx.pos].token_type == TokenType.LPAREN {
            ctx.pos++;
            mut depth: i32 = 1;
            loop {
                if ctx.pos >= len(ctx.tokens) {
                    break;
                }
                if ctx.tokens.data[ctx.pos].token_type == TokenType.LPAREN {
                    depth = depth + 1;
                }
                if ctx.tokens.data[ctx.pos].token_type == TokenType.RPAREN {
                    depth = depth - 1;
                    if depth == 0 {
                        ctx.pos++;
                        break;
                    }
                }
                ctx.pos++;
            }
        }
        
        skip_whitespace(ctx);
        if ctx.pos < len(ctx.tokens) and ctx.tokens.data[ctx.pos].token_type == TokenType.COLON {
            ctx.pos++;
            loop {
                if ctx.pos >= len(ctx.tokens) {
                    break;
                }
                if ctx.tokens.data[ctx.pos].token_type == TokenType.LBRACE {
                    break;
                }
                ctx.pos++;
            }
        }
        
        skip_whitespace(ctx);
        if ctx.pos < len(ctx.tokens) and ctx.tokens.data[ctx.pos].token_type == TokenType.LBRACE {
            ctx.pos++;
            mut depth: i32 = 1;
            loop {
                if ctx.pos >= len(ctx.tokens) {
                    break;
                }
                if ctx.tokens.data[ctx.pos].token_type == TokenType.LBRACE {
                    depth = depth + 1;
                }
                if ctx.tokens.data[ctx.pos].token_type == TokenType.RBRACE {
                    depth = depth - 1;
                    if depth == 0 {
                        ctx.pos++;
                        break;
                    }
                }
                ctx.pos++;
            }
        }
        return;
    }
    
    // Handle MODEL definitions
    if token_type == TokenType.MODEL {
        ctx.pos++;
        skip_whitespace(ctx);
        
        if ctx.pos < len(ctx.tokens) and ctx.tokens.data[ctx.pos].token_type == TokenType.IDENTIFIER {
            ctx.pos++;
        }
        
        skip_whitespace(ctx);
        if ctx.pos < len(ctx.tokens) and ctx.tokens.data[ctx.pos].token_type == TokenType.LBRACE {
            ctx.pos++;
            mut depth: i32 = 1;
            loop {
                if ctx.pos >= len(ctx.tokens) {
                    break;
                }
                if ctx.tokens.data[ctx.pos].token_type == TokenType.LBRACE {
                    depth = depth + 1;
                }
                if ctx.tokens.data[ctx.pos].token_type == TokenType.RBRACE {
                    depth = depth - 1;
                    if depth == 0 {
                        ctx.pos++;
                        break;
                    }
                }
                ctx.pos++;
            }
        }
        return;
    }
    
    // Handle ENUM definitions
    if token_type == TokenType.ENUM {
        ctx.pos++;  // Skip 'enum'
        skip_whitespace(ctx);
        
        if ctx.pos < len(ctx.tokens) and ctx.tokens.data[ctx.pos].token_type == TokenType.IDENTIFIER {
            ctx.pos++;
        }
        
        skip_whitespace(ctx);
        if ctx.pos < len(ctx.tokens) and ctx.tokens.data[ctx.pos].token_type == TokenType.LBRACE {
            ctx.pos++;
            mut depth: i32 = 1;
            loop {
                if ctx.pos >= len(ctx.tokens) {
                    break;
                }
                if ctx.tokens.data[ctx.pos].token_type == TokenType.LBRACE {
                    depth = depth + 1;
                }
                if ctx.tokens.data[ctx.pos].token_type == TokenType.RBRACE {
                    depth = depth - 1;
                    if depth == 0 {
                        ctx.pos++;
                        break;
                    }
                }
                ctx.pos++;
            }
        }
        return;
    }
    
    if token_type == TokenType.MUT {
        ctx.pos++;
        skip_whitespace(ctx);
        if ctx.pos < len(ctx.tokens) and ctx.tokens.data[ctx.pos].token_type == TokenType.VAL {
            ctx.pos++;  // Skip 'val'
        }
        loop {
            if ctx.pos >= len(ctx.tokens) {
                break;
            }
            if ctx.tokens.data[ctx.pos].token_type == TokenType.SEMICOLON {
                ctx.pos++;
                break;
            }
            ctx.pos++;
        }
        return;
    }
    
    // Handle VAL (immutable global variables)
    if token_type == TokenType.VAL {
        ctx.pos++;  // Skip 'val'
        // Skip until semicolon
        loop {
            if ctx.pos >= len(ctx.tokens) {
                break;
            }
            if ctx.tokens.data[ctx.pos].token_type == TokenType.SEMICOLON {
                ctx.pos++;
                break;
            }
            ctx.pos++;
        }
        return;
    }
    
    // Handle TEST blocks
    if token_type == TokenType.TEST {
        ctx.pos++;  // Skip 'test'
        skip_whitespace(ctx);
        
        // Skip test body
        if ctx.pos < len(ctx.tokens) and ctx.tokens.data[ctx.pos].token_type == TokenType.LBRACE {
            ctx.pos++;
            mut depth: i32 = 1;
            loop {
                if ctx.pos >= len(ctx.tokens) {
                    break;
                }
                if ctx.tokens.data[ctx.pos].token_type == TokenType.LBRACE {
                    depth = depth + 1;
                }
                if ctx.tokens.data[ctx.pos].token_type == TokenType.RBRACE {
                    depth = depth - 1;
                    if depth == 0 {
                        ctx.pos++;
                        break;
                    }
                }
                ctx.pos++;
            }
        }
        return;
    }
    
    // Handle PUB modifier
    if token_type == TokenType.PUB {
        ctx.pos++;  // Skip 'pub'
        skip_whitespace(ctx);
        // Recursively handle the next item
        parse_top_level(ctx, ast);
        return;
    }
    
    // Handle MACRO definitions
    if token_type == TokenType.MACRO {
        ctx.pos++;  // Skip 'macro'
        skip_whitespace(ctx);
        
        // Skip macro name
        if ctx.pos < len(ctx.tokens) and ctx.tokens.data[ctx.pos].token_type == TokenType.IDENTIFIER {
            ctx.pos++;
        }
        
        // Skip parameters
        skip_whitespace(ctx);
        if ctx.pos < len(ctx.tokens) and ctx.tokens.data[ctx.pos].token_type == TokenType.LPAREN {
            ctx.pos++;
            mut depth: i32 = 1;
            loop {
                if ctx.pos >= len(ctx.tokens) {
                    break;
                }
                if ctx.tokens.data[ctx.pos].token_type == TokenType.LPAREN {
                    depth = depth + 1;
                }
                if ctx.tokens.data[ctx.pos].token_type == TokenType.RPAREN {
                    depth = depth - 1;
                    if depth == 0 {
                        ctx.pos++;
                        break;
                    }
                }
                ctx.pos++;
            }
        }
        
        // Skip macro body
        skip_whitespace(ctx);
        if ctx.pos < len(ctx.tokens) and ctx.tokens.data[ctx.pos].token_type == TokenType.LBRACE {
            ctx.pos++;
            mut depth: i32 = 1;
            loop {
                if ctx.pos >= len(ctx.tokens) {
                    break;
                }
                if ctx.tokens.data[ctx.pos].token_type == TokenType.LBRACE {
                    depth = depth + 1;
                }
                if ctx.tokens.data[ctx.pos].token_type == TokenType.RBRACE {
                    depth = depth - 1;
                    if depth == 0 {
                        ctx.pos++;
                        break;
                    }
                }
                ctx.pos++;
            }
        }
        return;
    }
    
    // Handle OPAQUE type declarations
    if token_type == TokenType.OPAQUE {
        ctx.pos++;  // Skip 'opaque'
        loop {
            if ctx.pos >= len(ctx.tokens) {
                break;
            }
            if ctx.tokens.data[ctx.pos].token_type == TokenType.SEMICOLON {
                ctx.pos++;
                break;
            }
            ctx.pos++;
        }
        return;
    }
    
    // Handle PLATFORM blocks
    if token_type == TokenType.PLATFORM {
        ctx.pos++;  // Skip 'platform'
        skip_whitespace(ctx);
        
        // Skip platform name (windows/posix)
        if ctx.pos < len(ctx.tokens) {
            ctx.pos++;
        }
        
        // Skip platform body
        skip_whitespace(ctx);
        if ctx.pos < len(ctx.tokens) and ctx.tokens.data[ctx.pos].token_type == TokenType.LBRACE {
            ctx.pos++;
            mut depth: i32 = 1;
            loop {
                if ctx.pos >= len(ctx.tokens) {
                    break;
                }
                if ctx.tokens.data[ctx.pos].token_type == TokenType.LBRACE {
                    depth = depth + 1;
                }
                if ctx.tokens.data[ctx.pos].token_type == TokenType.RBRACE {
                    depth = depth - 1;
                    if depth == 0 {
                        ctx.pos++;
                        break;
                    }
                }
                ctx.pos++;
            }
        }
        return;
    }
    
    // Unknown token type - skip it
    ctx.pos++;
}

/// Skip the whitespace and newline tokens
def skip_whitespace(ctx: ref ParserContext) {
    loop {
        if ctx.pos >= len(ctx.tokens) {
            break;
        }
        
        val token_type = ctx.tokens.data[ctx.pos].token_type;
        
        if token_type == TokenType.WHITESPACE or token_type == TokenType.NEWLINE {
            ctx.pos++;
        } else {
            break;
        }
    }
}

/// Peek at current token without advancing
def peek(ctx: ref ParserContext): Token {
    if ctx.pos < len(ctx.tokens) {
        return ctx.tokens.data[ctx.pos];
    }
    
    mut empty: Token;
    empty.token_type = TokenType.IDENTIFIER;
    empty.value = str("");
    empty.line = 0;
    empty.column = 0;
    return empty;
}

/// Consume current token and advance
def consume(ctx: ref ParserContext): Token {
    val token: Token = peek(ctx);
    ctx.pos = ctx.pos + 1;
    return token;
}

/// Check if current token matches expected type
def expect(ctx: ref ParserContext, expected_type: i32): bool {
    skip_whitespace(ctx);
    val token: Token = peek(ctx);
    return token.token_type == expected_type;
}

/// Parse a type specification (e.g., "i32", "ref string", "list(Token)")
def parse_type(ctx: ref ParserContext): string {
    skip_whitespace(ctx);
    
    if ctx.pos >= len(ctx.tokens) {
        println "ERROR: Expected type but reached end of tokens";
        return str("");
    }
    
    mut ref_prefix: string = str("");
    
    loop {
        val token: Token = peek(ctx);
        if token.token_type == TokenType.REF {
            ref_prefix = concat_c(ref_prefix, "ref ");
            consume(ctx);
            skip_whitespace(ctx);
        } else {
            break;
        }
    }
    
    mut type_name: string = str("");
    val token: Token = peek(ctx);
    
    // Handle 'model' keyword as type for anonymous models
    if token.token_type == TokenType.MODEL {
        consume(ctx);
        return concat_c(ref_prefix, "model");
    }
    
    if token.token_type == TokenType.IDENTIFIER {
        type_name = token.value;
        consume(ctx);
        
        // Handle list(ElementType) syntax
        if equals_c(type_name, "list") {
            skip_whitespace(ctx);
            
            if !expect(ctx, TokenType.LPAREN) {
                println "ERROR: Expected '(' after 'list'";
                return str("");
            }
            consume(ctx);
            
            skip_whitespace(ctx);
            val elem_token: Token = consume(ctx);
            val element_type: string = elem_token.value;
            
            skip_whitespace(ctx);
            if !expect(ctx, TokenType.RPAREN) {
                println "ERROR: Expected ')' after list element type";
                return str("");
            }
            consume(ctx);
            type_name = concat(element_type, str("[999]"));
        }

        // Handle array brackets [size]
        loop {
            if expect(ctx, TokenType.LBRACKET) {
                consume(ctx);
                type_name = concat(type_name, str("["));
                
                loop {
                    val tok: Token = peek(ctx);
                    if tok.token_type == TokenType.RBRACKET {
                        break;
                    }
                    type_name = concat(type_name, tok.value);
                    consume(ctx);
                }
                
                if expect(ctx, TokenType.RBRACKET) {
                    consume(ctx);
                    type_name = concat(type_name, str("]"));
                }
            } else {
                break;
            }
        }
        
        // Handle pointer syntax *
        loop {
            val tok: Token = peek(ctx);
            if tok.token_type == TokenType.OPERATOR and equals_c(tok.value, "*") {
                consume(ctx);
                type_name = concat(type_name, str("*"));
            } else {
                break;
            }
        }
    } else {
        println "ERROR: Invalid type specification";
        return str("");
    }
    
    // TODO: Check type aliases
    // if type_name in g_type_aliases {
    //     type_name = g_type_aliases[type_name]
    // }
    
    return concat(ref_prefix, type_name);
}

/// Parse ref depth (e.g., "ref ref int" returns 2)
def parse_ref_depth(ctx: ref ParserContext): i32 {
    mut depth: i32 = 0;
    
    loop {
        skip_whitespace(ctx);
        
        if ctx.pos >= len(ctx.tokens) {
            break;
        }
        
        val token: Token = peek(ctx);
        if token.token_type == TokenType.REF {
            depth = depth + 1;
            consume(ctx);
        } else {
            break;
        }
    }
    
    return depth;
}

/// Parse function definition
def parse_function(ctx: ref ParserContext): ASTNode {
    mut func: ASTNode;
    func.node_type = str("Function");
    
    skip_whitespace(ctx);
    if ctx.pos >= len(ctx.tokens) or ctx.tokens.data[ctx.pos].token_type != TokenType.IDENTIFIER {
        println "ERROR: Expected function name after 'def'";
        return func;
    }
    
    val func_name: string = ctx.tokens.data[ctx.pos].value;
    ctx.pos++;
    
    skip_whitespace(ctx);
    if !expect(ctx, TokenType.LPAREN) {
        println "ERROR: Expected '(' after function name";
        return func;
    }
    consume(ctx);
    
    mut depth: i32 = 1;
    loop {
        if ctx.pos >= len(ctx.tokens) {
            break;
        }
        
        if ctx.tokens.data[ctx.pos].token_type == TokenType.LPAREN {
            depth = depth + 1;
        } elif ctx.tokens.data[ctx.pos].token_type == TokenType.RPAREN {
            depth = depth - 1;
            if depth == 0 {
                ctx.pos++;
                break;
            }
        }
        ctx.pos++;
    }
    
    skip_whitespace(ctx);
    if expect(ctx, TokenType.COLON) {
        consume(ctx);
        val return_type: string = parse_type(ctx);
    }
    
    skip_whitespace(ctx);
    if !expect(ctx, TokenType.LBRACE) {
        println "ERROR: Expected '{' after function declaration";
        return func;
    }
    consume(ctx);
    
    depth = 1;
    loop {
        if ctx.pos >= len(ctx.tokens) {
            break;
        }
        
        if ctx.tokens.data[ctx.pos].token_type == TokenType.LBRACE {
            depth = depth + 1;
        } elif ctx.tokens.data[ctx.pos].token_type == TokenType.RBRACE {
            depth = depth - 1;
            if depth == 0 {
                ctx.pos++;
                break;
            }
        }
        ctx.pos++;
    }
    
    return func;
}

// // TODO:                Implement remaining parser functions
// // - parse_model:       Parse model definitions
// // - parse_enum:        Parse enum definitions
// // - parse_statement:   Parse statements in function bodies
// // - parse_expression:  Parse expressions
// // - parse_block:       Parse { ... } blocks

// def main() {
//     println "Parser module loaded";
//     println "TODO: Implement remaining parser functions";
// }

test {
    println "\nTest 1: parse_type with simple type";
    mut tokens1: list(Token) = lex(str("i32"));
    mut ctx1: ParserContext;
    ctx1.tokens = addr_of(tokens1);
    ctx1.pos = 0;
    mut type1: string = parse_type(addr_of(ctx1));
    assert equals_c(type1, "i32"), "Expected i32.";

    println "\nTest 2: skip_whitespace";
    mut tokens2: list(Token) = lex(str("   \n  \t  i32"));
    mut ctx2: ParserContext;
    ctx2.tokens = addr_of(tokens2);
    ctx2.pos = 0;
    mut pos_before: i32 = ctx2.pos;
    skip_whitespace(addr_of(ctx2));
    assert ctx2.pos > pos_before, "Expected position to advance after skipping whitespace";

    println "\nTest 3: peek and consume";
    mut tokens3: list(Token) = lex(str("identifier"));
    mut ctx3: ParserContext;
    ctx3.tokens = addr_of(tokens3);
    ctx3.pos = 0;
    mut peeked: Token = peek(addr_of(ctx3));
    mut pos_after_peek: i32 = ctx3.pos;
    mut consumed: Token = consume(addr_of(ctx3));
    mut pos_after_consume: i32 = ctx3.pos;
    assert pos_after_peek == 0 and pos_after_consume == 1, "Expected position to advance after consume";

    println "\nTest 4: expect";
    mut tokens4: list(Token) = lex(str("def function_name"));
    mut ctx4: ParserContext;
    ctx4.tokens = addr_of(tokens4);
    ctx4.pos = 0;
    assert expect(addr_of(ctx4), TokenType.DEF), "Expected to find 'def' token";

    println "\nTest 5: parse_type with ref type";
    mut tokens5: list(Token) = lex(str("ref string"));
    mut ctx5: ParserContext;
    ctx5.tokens = addr_of(tokens5);
    ctx5.pos = 0;
    mut type5: string = parse_type(addr_of(ctx5));
    assert str_len(type5) > 0, "Expected non-empty type string for 'ref string'";
    assert str_contains_c(type5, "string"), "Expected type to contain 'string'";

    println "\nTest 6: parse_type with list type";
    mut tokens6: list(Token) = lex(str("list(Token)"));
    mut ctx6: ParserContext;
    ctx6.tokens = addr_of(tokens6);
    ctx6.pos = 0;
    mut type6: string = parse_type(addr_of(ctx6));
    assert str_len(type6) > 0, "Expected non-empty type for list(Token)";
    assert str_contains_c(type6, "Token"), "Expected type to contain 'Token'";

    println "\nTest 7: parse simple function with parse_function";
    mut tokens7: list(Token) = lex(str("get_value(): i32 { return 42; }"));
    mut ctx7: ParserContext;
    ctx7.tokens = addr_of(tokens7);
    ctx7.pos = 0;
    mut func7: ASTNode = parse_function(addr_of(ctx7));
    assert equals_c(func7.node_type, "Function"), "Expected Function node type";

    println "\nTest 8: parse function with parameters";
    mut tokens8: list(Token) = lex(str("add(a: i32, b: i32): i32 { return a + b; }"));
    mut ctx8: ParserContext;
    ctx8.tokens = addr_of(tokens8);
    ctx8.pos = 0;
    mut func8: ASTNode = parse_function(addr_of(ctx8));
    assert equals_c(func8.node_type, "Function"), "Expected Function node for function with params";

    println "\nTest 9: parse full program with def main";
    mut tokens9: list(Token) = lex(str("def main() { println \"Hello\"; }"));
    mut ast9: ASTNode = parse(addr_of(tokens9), false, false, str(""));
    assert equals_c(ast9.node_type, "Program"), "Expected Program AST node";

    println "\nTest 10: parse program with model definition";
    mut tokens10: list(Token) = lex(str("model Point { x: i32, y: i32 } def main() { }"));
    mut ast10: ASTNode = parse(addr_of(tokens10), false, false, str(""));
    assert equals_c(ast10.node_type, "Program"), "Expected Program node with model";

    println "\nTest 11: parse program with enum definition";
    mut tokens11: list(Token) = lex(str("enum Color { Red, Green, Blue } def main() { }"));
    mut ast11: ASTNode = parse(addr_of(tokens11), false, false, str(""));
    assert equals_c(ast11.node_type, "Program"), "Expected Program node with enum";

    println "\nTest 12: parse program with use statement";
    mut tokens12: list(Token) = lex(str("use std.io; def main() { }"));
    mut ast12: ASTNode = parse(addr_of(tokens12), false, false, str(""));
    assert equals_c(ast12.node_type, "Program"), "Expected Program node with use statement";

    println "\nTest 13: parse program with global val";
    mut tokens13: list(Token) = lex(str("val CONSTANT: i32 = 42; def main() { }"));
    mut ast13: ASTNode = parse(addr_of(tokens13), false, false, str(""));
    assert equals_c(ast13.node_type, "Program"), "Expected Program node with global val";

    println "\nTest 14: parse program with mut val";
    mut tokens14: list(Token) = lex(str("mut val counter: i32 = 0; def main() { }"));
    mut ast14: ASTNode = parse(addr_of(tokens14), false, false, str(""));
    assert equals_c(ast14.node_type, "Program"), "Expected Program node with mut val";

    println "\nTest 15: parse program with pub function";
    mut tokens15: list(Token) = lex(str("pub def helper() { } def main() { }"));
    mut ast15: ASTNode = parse(addr_of(tokens15), false, false, str(""));
    assert equals_c(ast15.node_type, "Program"), "Expected Program node with pub function";

    println "\nTest 16: parse empty test block";
    mut tokens16: list(Token) = lex(str("test { }"));
    mut ast16: ASTNode = parse(addr_of(tokens16), false, false, str(""));
    assert equals_c(ast16.node_type, "Program"), "Expected Program node with test block";

    println "\nTest 17: parse_type with pointer syntax";
    mut tokens17: list(Token) = lex(str("char*"));
    mut ctx17: ParserContext;
    ctx17.tokens = addr_of(tokens17);
    ctx17.pos = 0;
    mut type17: string = parse_type(addr_of(ctx17));
    assert str_len(type17) > 0, "Expected non-empty type for char*";
    assert substring(type17, str("char")) != 0, "Expected type to contain 'char'";

    println "\nTest 18: parse multiple functions";
    mut tokens18: list(Token) = lex(str("def foo() { } def bar() { } def main() { }"));
    mut ast18: ASTNode = parse(addr_of(tokens18), false, false, str(""));
    assert equals_c(ast18.node_type, "Program"), "Expected Program node with multiple functions";

    println "\nAll tests completed!";
}
